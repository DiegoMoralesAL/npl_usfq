{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "uiCDtW2w-hyf"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from sklearn.metrics import accuracy_score\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import nltk\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## SIN PREPROCESAMIENTO"
   ],
   "metadata": {
    "id": "ULv09gXWLXPy"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# Cargar el dataset desde un archivo .tsv\n",
    "data = pd.read_csv('train.tsv', sep='\\t')\n",
    "\n",
    "# Rellenar valores faltantes con un string vacío\n",
    "data['Phrase'] = data['Phrase'].fillna('')\n",
    "\n",
    "# Exploratory Data Analysis (EDA)\n",
    "print(data['Sentiment'].value_counts())\n",
    "print(data.head())\n",
    "\n",
    "\n",
    "# Aplicar preprocesamiento al dataset\n",
    "data['Processed_Review'] = data['Phrase']\n",
    "\n",
    "# Separar datos en características y etiquetas\n",
    "X = data['Processed_Review']\n",
    "y = data['Sentiment']\n",
    "\n",
    "# Codificación de etiquetas\n",
    "label_encoder = LabelEncoder()\n",
    "y = label_encoder.fit_transform(y)\n",
    "\n",
    "# Dividir el dataset en entrenamiento y prueba\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Crear el pipeline para el modelo de Naive Bayes\n",
    "model = Pipeline([\n",
    "    ('vectorizer', CountVectorizer()),\n",
    "    ('classifier', MultinomialNB())\n",
    "])\n",
    "\n",
    "# Entrenar el modelo\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Diccionario para mapear las etiquetas numéricas a texto\n",
    "label_map = {\n",
    "    0: 'negative',\n",
    "    1: 'somewhat negative',\n",
    "    2: 'neutral',\n",
    "    3: 'somewhat positive',\n",
    "    4: 'positive'\n",
    "}\n",
    "\n",
    "# Evaluar el modelo en el conjunto de prueba (X_test)\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Mantén las predicciones y etiquetas verdaderas en formato numérico para classification_report\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Calcular el accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy del modelo en el conjunto de prueba: {accuracy:.4f}\")\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3Fo_mRG2LW4n",
    "outputId": "6fa9c766-0a68-4c2a-a56b-9f294a876df1"
   },
   "execution_count": 2,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Sentiment\n",
      "2    79582\n",
      "3    32927\n",
      "1    27273\n",
      "4     9206\n",
      "0     7072\n",
      "Name: count, dtype: int64\n",
      "   PhraseId  SentenceId                                             Phrase  \\\n",
      "0         1           1  A series of escapades demonstrating the adage ...   \n",
      "1         2           1  A series of escapades demonstrating the adage ...   \n",
      "2         3           1                                           A series   \n",
      "3         4           1                                                  A   \n",
      "4         5           1                                             series   \n",
      "\n",
      "   Sentiment  \n",
      "0          1  \n",
      "1          2  \n",
      "2          2  \n",
      "3          2  \n",
      "4          2  \n",
      "[[  416   712   249    35     4]\n",
      " [  382  2444  2391   286    24]\n",
      " [  122  1430 12232  1709   146]\n",
      " [   17   283  2541  3359   507]\n",
      " [    2    27   272   977   645]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.44      0.29      0.35      1416\n",
      "           1       0.50      0.44      0.47      5527\n",
      "           2       0.69      0.78      0.73     15639\n",
      "           3       0.53      0.50      0.51      6707\n",
      "           4       0.49      0.34      0.40      1923\n",
      "\n",
      "    accuracy                           0.61     31212\n",
      "   macro avg       0.53      0.47      0.49     31212\n",
      "weighted avg       0.60      0.61      0.60     31212\n",
      "\n",
      "Accuracy del modelo en el conjunto de prueba: 0.6118\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "##CON PREPROCESAMIENTO"
   ],
   "metadata": {
    "id": "Fxt2XhmoOqiN"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "\n",
    "# Descargar recursos necesarios para nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# Cargar el dataset desde un archivo .tsv\n",
    "data = pd.read_csv('train.tsv', sep='\\t')\n",
    "\n",
    "# Rellenar valores faltantes con un string vacío\n",
    "data['Phrase'] = data['Phrase'].fillna('')\n",
    "\n",
    "# Exploratory Data Analysis (EDA)\n",
    "print(data['Sentiment'].value_counts())\n",
    "print(data.head())\n",
    "\n",
    "# Definir funciones de preprocesamiento\n",
    "def preprocess_text(text, remove_stopwords=True, use_stemming=False, use_lemmatization=False):\n",
    "    # Convertir a minúsculas\n",
    "    text = text.lower()\n",
    "\n",
    "    # Tokenización\n",
    "    words = text.split()\n",
    "\n",
    "    # Eliminar stopwords\n",
    "    if remove_stopwords:\n",
    "        stop_words = set(stopwords.words('english'))\n",
    "        words = [word for word in words if word not in stop_words]\n",
    "\n",
    "    # Stemming\n",
    "    if use_stemming:\n",
    "        stemmer = PorterStemmer()\n",
    "        words = [stemmer.stem(word) for word in words]\n",
    "\n",
    "    # Lemmatization\n",
    "    if use_lemmatization:\n",
    "        lemmatizer = WordNetLemmatizer()\n",
    "        words = [lemmatizer.lemmatize(word) for word in words]\n",
    "\n",
    "    return ' '.join(words)\n",
    "\n",
    "# Aplicar preprocesamiento al dataset\n",
    "data['Processed_Review'] = data['Phrase'].apply(lambda x: preprocess_text(x, remove_stopwords=True, use_stemming=True, use_lemmatization=True))\n",
    "\n",
    "# Separar datos en características y etiquetas\n",
    "X = data['Processed_Review']\n",
    "y = data['Sentiment']\n",
    "\n",
    "# Codificación de etiquetas\n",
    "label_encoder = LabelEncoder()\n",
    "y = label_encoder.fit_transform(y)\n",
    "\n",
    "# Dividir el dataset en entrenamiento y prueba\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Crear el pipeline para el modelo de Naive Bayes\n",
    "model = Pipeline([\n",
    "    ('vectorizer', CountVectorizer(binary=False)),\n",
    "    ('classifier', MultinomialNB())\n",
    "])\n",
    "\n",
    "# Entrenar el modelo\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Diccionario para mapear las etiquetas numéricas a texto\n",
    "label_map = {\n",
    "    0: 'negative',\n",
    "    1: 'somewhat negative',\n",
    "    2: 'neutral',\n",
    "    3: 'somewhat positive',\n",
    "    4: 'positive'\n",
    "}\n",
    "\n",
    "# Evaluar el modelo en el conjunto de prueba (X_test)\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Mantén las predicciones y etiquetas verdaderas en formato numérico para classification_report\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Calcular el accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy del modelo en el conjunto de prueba: {accuracy:.4f}\")\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aNjlMXnGDwbO",
    "outputId": "72300752-2a70-4fd1-a9af-bf1b7979fbfa"
   },
   "execution_count": 3,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Sentiment\n",
      "2    79582\n",
      "3    32927\n",
      "1    27273\n",
      "4     9206\n",
      "0     7072\n",
      "Name: count, dtype: int64\n",
      "   PhraseId  SentenceId                                             Phrase  \\\n",
      "0         1           1  A series of escapades demonstrating the adage ...   \n",
      "1         2           1  A series of escapades demonstrating the adage ...   \n",
      "2         3           1                                           A series   \n",
      "3         4           1                                                  A   \n",
      "4         5           1                                             series   \n",
      "\n",
      "   Sentiment  \n",
      "0          1  \n",
      "1          2  \n",
      "2          2  \n",
      "3          2  \n",
      "4          2  \n",
      "[[  383   694   304    32     3]\n",
      " [  327  2188  2676   314    22]\n",
      " [   88  1159 12715  1558   119]\n",
      " [   14   212  2876  3196   409]\n",
      " [    0    22   335  1043   523]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.47      0.27      0.34      1416\n",
      "           1       0.51      0.40      0.45      5527\n",
      "           2       0.67      0.81      0.74     15639\n",
      "           3       0.52      0.48      0.50      6707\n",
      "           4       0.49      0.27      0.35      1923\n",
      "\n",
      "    accuracy                           0.61     31212\n",
      "   macro avg       0.53      0.45      0.47     31212\n",
      "weighted avg       0.59      0.61      0.59     31212\n",
      "\n",
      "Accuracy del modelo en el conjunto de prueba: 0.6089\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "##CONJUNTO DE TEST"
   ],
   "metadata": {
    "id": "YMMJhmoIO5j7"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# Cargar el dataset desde un archivo .tsv\n",
    "test_data = pd.read_csv('test.tsv', sep='\\t')\n",
    "\n",
    "# Rellenar valores faltantes con un string vacío\n",
    "test_data['Phrase'] = test_data['Phrase'].fillna('')\n",
    "\n",
    "# Aplicar preprocesamiento al dataset\n",
    "test_data['Processed_Review'] = test_data['Phrase'].apply(lambda x: preprocess_text(x, remove_stopwords=True, use_stemming=True, use_lemmatization=True))\n",
    "\n",
    "# Generar predicciones en el dataset de prueba externo (test_data)\n",
    "X_test_final = test_data['Processed_Review']\n",
    "test_predictions = model.predict(X_test_final)\n",
    "\n",
    "# Calcular el accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy del modelo en el conjunto de prueba: {accuracy:.4f}\")\n",
    "\n",
    "# Convertir predicciones numéricas a etiquetas de texto\n",
    "test_predictions_text = [label_map[pred] for pred in test_predictions]\n",
    "\n",
    "# Agregar las predicciones al dataframe de prueba externo\n",
    "test_data['Predicted_Sentiment'] = test_predictions_text\n",
    "print(test_data[['Phrase', 'Predicted_Sentiment']].head(10))"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5X_n98BhEVFK",
    "outputId": "694d343b-8d31-44b7-e478-2d058907dd70"
   },
   "execution_count": 5,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Accuracy del modelo en el conjunto de prueba: 0.6089\n",
      "                                              Phrase Predicted_Sentiment\n",
      "0  An intermittently pleasing but mostly routine ...   somewhat positive\n",
      "1  An intermittently pleasing but mostly routine ...   somewhat positive\n",
      "2                                                 An             neutral\n",
      "3  intermittently pleasing but mostly routine effort   somewhat positive\n",
      "4         intermittently pleasing but mostly routine   somewhat positive\n",
      "5                        intermittently pleasing but   somewhat positive\n",
      "6                            intermittently pleasing   somewhat positive\n",
      "7                                     intermittently   somewhat positive\n",
      "8                                           pleasing   somewhat positive\n",
      "9                                                but             neutral\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Estas técnicas de preprocesamiento, en conjunto, tienden a:\n",
    "\n",
    "Reducir la dimensionalidad: Al eliminar palabras irrelevantes (como stop words) y normalizar el texto (a través de case folding, stemming, y lemmatization), el modelo se entrena con un conjunto de datos más conciso y manejable.\n",
    "Mejorar la precisión: Al enfocarse en las palabras más significativas y su frecuencia de aparición, el modelo puede aprender patrones más claros y diferenciados, lo que suele resultar en una mejora de la precisión.\n",
    "Aumentar la eficiencia: Reducir el ruido y la dimensionalidad no solo mejora la precisión sino que también hace que el entrenamiento y la predicción sean más rápidos y menos costosos en términos de recursos computacionales."
   ],
   "metadata": {
    "id": "mPdNsBs8TSEy"
   }
  }
 ]
}
